{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaForge Research Notebook ğŸš€\n",
    "\n",
    "**Interactive environment for systematic alpha research and factor model validation**\n",
    "\n",
    "This notebook demonstrates the full capabilities of AlphaForge for quantitative factor research, portfolio construction, and performance analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Factor Analysis](#factors)\n",
    "3. [Portfolio Construction](#portfolio)\n",
    "4. [Backtesting](#backtest)\n",
    "5. [Walk-Forward Analysis](#walkforward)\n",
    "6. [Performance Analysis](#performance)\n",
    "7. [Advanced Techniques](#advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import AlphaForge framework\n",
    "from factor_backtester import (\n",
    "    Backtester, BacktestConfig, DataProvider, \n",
    "    FactorCalculator, PortfolioConstructor, \n",
    "    PerformanceAnalyzer\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"ğŸš€ AlphaForge Research Environment Ready!\")\n",
    "print(\"ğŸ“Š Systematic alpha research toolkit loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaForge Configuration\n",
    "config = BacktestConfig(\n",
    "    start_date=\"2015-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    rebalance_freq=\"M\",  # Monthly rebalancing\n",
    "    transaction_cost=0.001,  # 10 bps transaction cost\n",
    "    max_weight=0.05,  # 5% max position\n",
    "    min_weight=-0.05,  # 5% max short position\n",
    "    leverage=1.0  # No leverage\n",
    ")\n",
    "\n",
    "# Initialize AlphaForge components\n",
    "data_provider = DataProvider()\n",
    "backtester = Backtester(config)\n",
    "\n",
    "print(f\"âš™ï¸ AlphaForge Configuration:\")\n",
    "print(f\"   ğŸ“… Period: {config.start_date} to {config.end_date}\")\n",
    "print(f\"   ğŸ”„ Rebalancing: {config.rebalance_freq}\")\n",
    "print(f\"   ğŸ’° Transaction Cost: {config.transaction_cost:.1%}\")\n",
    "print(f\"   ğŸ“Š Position Limits: {config.min_weight:.1%} to {config.max_weight:.1%}\")\n",
    "print(f\"   ğŸ¯ Leverage: {config.leverage}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market universe\n",
    "print(\"ğŸ“ˆ Loading S&P 500 universe...\")\n",
    "tickers = data_provider.get_universe(\"SP500\")\n",
    "print(f\"ğŸŒŸ Universe: {len(tickers)} stocks\")\n",
    "print(f\"ğŸ“‹ Sample tickers: {tickers[:15]}\")\n",
    "\n",
    "# Fetch market data with caching\n",
    "print(\"\\nğŸ”„ Fetching market data (this may take a moment)...\")\n",
    "raw_data = data_provider.fetch_yahoo_data(tickers, config.start_date, config.end_date)\n",
    "print(f\"âœ… Loaded {len(raw_data):,} observations\")\n",
    "print(f\"ğŸ“Š {len(raw_data['ticker'].unique())} unique tickers\")\n",
    "print(f\"ğŸ“… Date range: {raw_data['Date'].min()} to {raw_data['Date'].max()}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nğŸ“‹ Sample Data:\")\n",
    "display(raw_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Factor Analysis {#factors}\n",
    "\n",
    "Calculate and analyze classic risk factors using AlphaForge's factor engineering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate factors using AlphaForge\n",
    "print(\"ğŸ”¬ Calculating systematic risk factors...\")\n",
    "factor_calculator = FactorCalculator(raw_data)\n",
    "factor_data = factor_calculator.calculate_all_factors()\n",
    "\n",
    "print(f\"ğŸ“Š Factor data shape: {factor_data.shape}\")\n",
    "print(f\"ğŸ“… Factor coverage: {factor_data['Date'].min()} to {factor_data['Date'].max()}\")\n",
    "print(f\"ğŸ¯ {len(factor_data['ticker'].unique())} stocks with factor scores\")\n",
    "\n",
    "# Display factor summary statistics\n",
    "factor_cols = ['momentum', 'value', 'quality', 'size', 'low_vol']\n",
    "print(\"\\nğŸ“ˆ Factor Summary Statistics:\")\n",
    "factor_summary = factor_data[factor_cols].describe()\n",
    "display(factor_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor correlation and distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Factor correlation heatmap\n",
    "corr_matrix = factor_data[factor_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, ax=axes[0, 0], fmt='.3f')\n",
    "axes[0, 0].set_title('ğŸ“Š Factor Correlation Matrix')\n",
    "\n",
    "# Factor rank distributions\n",
    "factor_ranks = [col + '_rank' for col in factor_cols if col + '_rank' in factor_data.columns]\n",
    "if factor_ranks:\n",
    "    factor_data[factor_ranks].hist(bins=50, ax=axes[0, 1], alpha=0.7)\n",
    "    axes[0, 1].set_title('ğŸ“ˆ Factor Rank Distributions')\n",
    "\n",
    "# Factor stability over time (cross-sectional means)\n",
    "factor_ts = factor_data.groupby('Date')[factor_cols].mean()\n",
    "for factor in factor_cols:\n",
    "    axes[1, 0].plot(factor_ts.index, factor_ts[factor], label=factor.title(), alpha=0.8)\n",
    "axes[1, 0].set_title('ğŸ”„ Factor Evolution Over Time')\n",
    "axes[1, 0].set_ylabel('Cross-Sectional Mean')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Factor volatility\n",
    "factor_vol = factor_data.groupby('Date')[factor_cols].std().mean()\n",
    "factor_vol.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_title('ğŸ“Š Average Factor Volatility')\n",
    "axes[1, 1].set_ylabel('Cross-Sectional Std Dev')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Portfolio Construction {#portfolio}\n",
    "\n",
    "Demonstrate systematic portfolio construction with factor-based signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio construction example\n",
    "portfolio_constructor = PortfolioConstructor(config)\n",
    "\n",
    "# Select a sample date for analysis\n",
    "sample_date = factor_data['Date'].iloc[len(factor_data)//2]  # Mid-sample date\n",
    "print(f\"ğŸ“… Portfolio construction date: {sample_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Construct portfolio with AlphaForge\n",
    "weights = portfolio_constructor.construct_portfolio(\n",
    "    factor_data, sample_date, use_shrinkage=True, use_lasso=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Portfolio Statistics:\")\n",
    "print(f\"   ğŸ¯ Total positions: {len(weights)}\")\n",
    "print(f\"   ğŸ“ˆ Long positions: {(weights > 0).sum()}\")\n",
    "print(f\"   ğŸ“‰ Short positions: {(weights < 0).sum()}\")\n",
    "print(f\"   ğŸ’° Total long weight: {weights[weights > 0].sum():.2%}\")\n",
    "print(f\"   ğŸ’¸ Total short weight: {weights[weights < 0].sum():.2%}\")\n",
    "print(f\"   ğŸª Net exposure: {weights.sum():.2%}\")\n",
    "print(f\"   ğŸŒ Gross exposure: {weights.abs().sum():.2%}\")\n",
    "\n",
    "# Display top holdings\n",
    "if len(weights) > 0:\n",
    "    print(\"\\nğŸ” Top 10 Long Positions:\")\n",
    "    display(weights.nlargest(10).to_frame('Weight'))\n",
    "    \n",
    "    print(\"\\nğŸ”» Top 10 Short Positions:\")\n",
    "    display(weights.nsmallest(10).to_frame('Weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio construction\n",
    "if len(weights) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Weight distribution\n",
    "    axes[0, 0].hist(weights, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('ğŸ“Š Portfolio Weight Distribution')\n",
    "    axes[0, 0].set_xlabel('Weight')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Long vs Short exposure\n",
    "    long_weights = weights[weights > 0]\n",
    "    short_weights = weights[weights < 0]\n",
    "    exposure_data = ['Long', 'Short']\n",
    "    exposure_values = [long_weights.sum(), abs(short_weights.sum())]\n",
    "    colors = ['green', 'red']\n",
    "    \n",
    "    axes[0, 1].bar(exposure_data, exposure_values, color=colors, alpha=0.7)\n",
    "    axes[0, 1].set_title('ğŸ“ˆ Long vs Short Exposure')\n",
    "    axes[0, 1].set_ylabel('Total Weight')\n",
    "    \n",
    "    # Top positions by absolute weight\n",
    "    top_positions = weights.abs().nlargest(15)\n",
    "    position_colors = ['green' if weights[ticker] > 0 else 'red' for ticker in top_positions.index]\n",
    "    \n",
    "    y_pos = range(len(top_positions))\n",
    "    axes[1, 0].barh(y_pos, top_positions.values, color=position_colors, alpha=0.7)\n",
    "    axes[1, 0].set_yticks(y_pos)\n",
    "    axes[1, 0].set_yticklabels(top_positions.index, fontsize=8)\n",
    "    axes[1, 0].set_title('ğŸ¯ Top 15 Positions by Absolute Weight')\n",
    "    axes[1, 0].set_xlabel('Absolute Weight')\n",
    "    \n",
    "    # Portfolio utilization\n",
    "    gross_exposure = weights.abs().sum()\n",
    "    cash_allocation = 1 - gross_exposure\n",
    "    \n",
    "    utilization_labels = ['Invested', 'Cash']\n",
    "    utilization_values = [gross_exposure, cash_allocation]\n",
    "    \n",
    "    axes[1, 1].pie(utilization_values, labels=utilization_labels, \n",
    "                   autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgray'])\n",
    "    axes[1, 1].set_title('ğŸ’° Portfolio Utilization')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No portfolio weights generated for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backtesting {#backtest}\n",
    "\n",
    "Execute comprehensive backtesting with transaction costs and performance analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive backtest\n",
    "print(\"ğŸš€ Running AlphaForge backtest...\")\n",
    "print(\"   ğŸ”¬ Applying Bayesian shrinkage and Lasso regularization\")\n",
    "print(\"   ğŸ’° Including realistic transaction costs\")\n",
    "print(\"   ğŸ“Š Computing comprehensive performance metrics\")\n",
    "\n",
    "results = backtester.run_backtest(tickers=tickers, use_shrinkage=True, use_lasso=True)\n",
    "\n",
    "if results:\n",
    "    print(\"\\nâœ… Backtest completed successfully!\")\n",
    "    print(\"\\nğŸ“Š Performance Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Total Return:        {metrics['total_return']:>8.2%}\")\n",
    "    print(f\"ğŸ“Š Annualized Return:   {metrics['annualized_return']:>8.2%}\")\n",
    "    print(f\"ğŸ“‰ Volatility:          {metrics['volatility']:>8.2%}\")\n",
    "    print(f\"âš¡ Sharpe Ratio:        {metrics['sharpe_ratio']:>8.2f}\")\n",
    "    print(f\"ğŸ”» Maximum Drawdown:    {metrics['max_drawdown']:>8.2%}\")\n",
    "    print(f\"ğŸ¯ Win Rate:            {metrics['win_rate']:>8.2%}\")\n",
    "    print(f\"ğŸ“ Skewness:            {metrics['skewness']:>8.2f}\")\n",
    "    print(f\"ğŸ“Š Kurtosis:            {metrics['kurtosis']:>8.2f}\")\n",
    "    print(f\"ğŸ“‹ Observations:        {metrics['num_observations']:>8d}\")\n",
    "    \n",
    "    # Transaction cost analysis\n",
    "    total_costs = results['transaction_costs'].sum()\n",
    "    print(f\"\\nğŸ’¸ Transaction Cost Analysis:\")\n",
    "    print(f\"   Total Costs:         {total_costs:.4f}\")\n",
    "    print(f\"   Average per Period:  {results['transaction_costs'].mean():.4f}\")\n",
    "    print(f\"   Annual Cost Drag:    {total_costs / len(results['returns']) * 252:.2%}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Backtest failed - check data availability and configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance visualization\n",
    "if results:\n",
    "    returns = results['returns']\n",
    "    gross_returns = results['gross_returns']\n",
    "    transaction_costs = results['transaction_costs']\n",
    "    \n",
    "    # Create performance dashboard\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('ğŸ“Š AlphaForge Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Cumulative returns\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    cum_gross_returns = (1 + gross_returns).cumprod()\n",
    "    \n",
    "    axes[0, 0].plot(cum_returns.index, cum_returns.values, \n",
    "                    label='Net Returns', linewidth=2.5, color='steelblue')\n",
    "    axes[0, 0].plot(cum_gross_returns.index, cum_gross_returns.values, \n",
    "                    label='Gross Returns', linewidth=2, alpha=0.7, color='orange')\n",
    "    axes[0, 0].set_title('ğŸ“ˆ Cumulative Returns')\n",
    "    axes[0, 0].set_ylabel('Cumulative Return')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Rolling Sharpe ratio\n",
    "    rolling_sharpe = returns.rolling(60).mean() / returns.rolling(60).std() * np.sqrt(252)\n",
    "    axes[0, 1].plot(rolling_sharpe.index, rolling_sharpe.values, \n",
    "                    color='green', linewidth=2)\n",
    "    axes[0, 1].set_title('âš¡ Rolling Sharpe Ratio (60-day)')\n",
    "    axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].axhline(y=1, color='green', linestyle='--', alpha=0.5, label='Target')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Drawdown analysis\n",
    "    rolling_max = cum_returns.expanding().max()\n",
    "    drawdown = (cum_returns / rolling_max) - 1\n",
    "    axes[0, 2].fill_between(drawdown.index, drawdown.values, 0, \n",
    "                           alpha=0.4, color='red', label='Drawdown')\n",
    "    axes[0, 2].plot(drawdown.index, drawdown.values, color='darkred', linewidth=1)\n",
    "    axes[0, 2].set_title('ğŸ”» Drawdown Analysis')\n",
    "    axes[0, 2].set_ylabel('Drawdown')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Return distribution\n",
    "    axes[1, 0].hist(returns, bins=50, alpha=0.7, color='lightblue', \n",
    "                    edgecolor='black', density=True)\n",
    "    axes[1, 0].axvline(returns.mean(), color='red', linestyle='--', \n",
    "                       alpha=0.8, linewidth=2, label=f'Mean: {returns.mean():.3f}')\n",
    "    axes[1, 0].axvline(returns.median(), color='green', linestyle='--', \n",
    "                       alpha=0.8, linewidth=2, label=f'Median: {returns.median():.3f}')\n",
    "    axes[1, 0].set_title('ğŸ“Š Return Distribution')\n",
    "    axes[1, 0].set_xlabel('Daily Return')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Transaction costs over time\n",
    "    axes[1, 1].plot(transaction_costs.index, transaction_costs.cumsum(), \n",
    "                    color='purple', linewidth=2)\n",
    "    axes[1, 1].set_title('ğŸ’¸ Cumulative Transaction Costs')\n",
    "    axes[1, 1].set_ylabel('Cumulative Costs')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Rolling volatility\n",
    "    rolling_vol = returns.rolling(60).std() * np.sqrt(252)\n",
    "    axes[1, 2].plot(rolling_vol.index, rolling_vol.values, \n",
    "                    color='orange', linewidth=2)\n",
    "    axes[1, 2].set_title('ğŸ“‰ Rolling Volatility (60-day)')\n",
    "    axes[1, 2].set_ylabel('Annualized Volatility')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ“Š Performance dashboard generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walk-Forward Analysis {#walkforward}\n",
    "\n",
    "Rigorous out-of-sample testing with expanding windows for unbiased performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward out-of-sample analysis\n",
    "print(\"ğŸ”„ Running walk-forward analysis...\")\n",
    "print(\"   ğŸ“ˆ Expanding window approach for robust validation\")\n",
    "print(\"   ğŸ¯ Out-of-sample performance estimation\")\n",
    "print(\"   âš¡ Statistical significance testing\")\n",
    "\n",
    "oos_results = backtester.walk_forward_analysis(\n",
    "    tickers=tickers,\n",
    "    initial_window=504,  # 2 years initial training\n",
    "    step_size=21  # Monthly steps\n",
    ")\n",
    "\n",
    "if oos_results:\n",
    "    print(\"\\nâœ… Walk-forward analysis completed!\")\n",
    "    print(\"\\nğŸ“Š Out-of-Sample Results:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    oos_metrics = oos_results['oos_metrics']\n",
    "    \n",
    "    print(f\"ğŸ“ˆ OOS Total Return:      {oos_metrics['total_return']:>8.2%}\")\n",
    "    print(f\"ğŸ“Š OOS Annualized Return: {oos_metrics['annualized_return']:>8.2%}\")\n",
    "    print(f\"ğŸ“‰ OOS Volatility:        {oos_metrics['volatility']:>8.2%}\")\n",
    "    print(f\"âš¡ OOS Sharpe Ratio:      {oos_metrics['sharpe_ratio']:>8.2f}\")\n",
    "    print(f\"ğŸ”» OOS Maximum Drawdown:  {oos_metrics['max_drawdown']:>8.2%}\")\n",
    "    print(f\"ğŸ¯ OOS Win Rate:          {oos_metrics['win_rate']:>8.2%}\")\n",
    "    print(f\"ğŸ“‹ OOS Observations:      {oos_metrics['num_observations']:>8d}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Walk-forward analysis failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare in-sample vs out-of-sample performance\n",
    "if results and oos_results:\n",
    "    print(\"ğŸ“Š In-Sample vs Out-of-Sample Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = {\n",
    "        'Metric': ['Total Return', 'Annualized Return', 'Volatility', 'Sharpe Ratio', 'Max Drawdown', 'Win Rate'],\n",
    "        'In-Sample': [\n",
    "            results['metrics']['total_return'],\n",
    "            results['metrics']['annualized_return'],\n",
    "            results['metrics']['volatility'],\n",
    "            results['metrics']['sharpe_ratio'],\n",
    "            results['metrics']['max_drawdown'],\n",
    "            results['metrics']['win_rate']\n",
    "        ],\n",
    "        'Out-of-Sample': [\n",
    "            oos_results['oos_metrics']['total_return'],\n",
    "            oos_results['oos_metrics']['annualized_return'],\n",
    "            oos_results['oos_metrics']['volatility'],\n",
    "            oos_results['oos_metrics']['sharpe_ratio'],\n",
    "            oos_results['oos_metrics']['max_drawdown'],\n",
    "            oos_results['oos_metrics']['win_rate']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Calculate degradation\n",
    "    comparison_df['Degradation (%)'] = (\n",
    "        (comparison_df['Out-of-Sample'] - comparison_df['In-Sample']) / \n",
    "        comparison_df['In-Sample'].abs() * 100\n",
    "    )\n",
    "    \n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Visualization comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Performance metrics comparison\n",
    "    metrics_to_plot = ['Annualized Return', 'Volatility', 'Sharpe Ratio']\n",
    "    x = np.arange(len(metrics_to_plot))\n", 
    "    width = 0.35\n"
    ]
    },

  
    {   
    "   source": [
    "   is_values = [comparison_df[comparison_df['Metric'] == m]['In-Sample'].iloc[0] for m in metrics_to_plot]\n",
    "   oos_values = [comparison_df[comparison_df['Metric'] == m]['Out-of-Sample'].iloc[0] for m in metrics_to_plot]\n",
    "   \n",
    "   axes[0].bar(x - width/2, is_values, width, label='In-Sample', alpha=0.8, color='steelblue')\n",
    "   axes[0].bar(x + width/2, oos_values, width, label='Out-of-Sample', alpha=0.8, color='orange')\n",
    "   axes[0].set_xlabel('Metrics')\n",
    "   axes[0].set_ylabel('Value')\n",
    "   axes[0].set_title('ğŸ“Š Performance Metrics Comparison')\n",
    "   axes[0].set_xticks(x)\n",
    "   axes[0].set_xticklabels(metrics_to_plot)\n",
    "   axes[0].legend()\n",
    "   axes[0].grid(True, alpha=0.3)\n",
    "   \n",
    "   # Cumulative returns comparison\n",
    "   if 'returns' in results and 'oos_returns' in oos_results:\n",
    "      is_cum = (1 + results['returns']).cumprod()\n",
    "       oos_cum = (1 + oos_results['oos_returns']).cumprod()\n",
    "   \n",
    "    axes[1].plot(is_cum.index, is_cum.values, label='In-Sample', linewidth=2.5, color='steelblue')\n",
    "    axes[1].plot(oos_cum.index, oos_cum.values, label='Out-of-Sample', linewidth=2.5, color='orange')\n",
    "    axes[1].set_title('ğŸ“ˆ Cumulative Returns Comparison')\n",
    "    axes[1].set_ylabel('Cumulative Return')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "   \n",
    "   # Degradation analysis\n",
    "   degradation_data = comparison_df[comparison_df['Metric'].isin(metrics_to_plot)]\n",
    "   colors = ['red' if x < 0 else 'green' for x in degradation_data['Degradation (%)']]\n",
    "   \n",
    "   axes[2].bar(range(len(degradation_data)), degradation_data['Degradation (%)'], \n",
    "           color=colors, alpha=0.7)\n",
    "   axes[2].set_title('ğŸ“‰ Performance Degradation')\n",
    "   axes[2].set_xticks(range(len(degradation_data)))\n",
    "   axes[2].set_xticklabels(degradation_data['Metric'], rotation=45)\n",
    "   axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "   axes[2].set_ylabel('Degradation (%)')\n",
    "   axes[2].grid(True, alpha=0.3)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n"
    ]
    },
    {
    "   source": [
    "   # Statistical significance test\n",
    "   if 'returns' in results and 'oos_returns' in oos_results:\n",
    "    from scipy import stats\n",
    "    \n",
    "    is_returns = results['returns'].dropna()\n",
    "    oos_returns = oos_results['oos_returns'].dropna()\n",
    "    \n",
    "    # T-test for mean difference\n",
    "    t_stat, p_value = stats.ttest_ind(is_returns, oos_returns)\n",
    "    \n",
    "    print(f'\\nğŸ”¬ Statistical Significance Test:')\n",
    "    print(f'   Mean Return Difference: {is_returns.mean() - oos_returns.mean():.4f}')\n",
    "    print(f'   T-statistic: {t_stat:.3f}')\n",
    "    print(f'   P-value: {p_value:.4f}')\n",
    "    print(f'   Significant difference: {\"Yes\" if p_value < 0.05 else \"No\"} (Î± = 0.05)')\n",
    "    \n",
    "    # Performance assessment\n",
    "    sharpe_degradation = comparison_df[comparison_df['Metric'] == 'Sharpe Ratio']['Degradation (%)'].iloc[0]\n",
    "    print(f'\\nğŸ¯ Strategy Assessment:')\n",
    "    if sharpe_degradation > -20:\n",
    "        print(f'   âœ… Excellent: Model shows robust out-of-sample performance')\n",
    "    elif sharpe_degradation > -50:\n",
    "        print(f'   âš ï¸  Good: Model shows acceptable degradation')\n",
    "    else:\n",
    "        print(f'   âŒ Poor: Model shows significant overfitting')\n",
    "    print(f'   ğŸ“Š Sharpe ratio degradation: {sharpe_degradation:.1f}%')\n"
    ]

    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 6. Performance Analysis {#performance}\\n\\nDeep dive into risk metrics, attribution analysis, and performance characteristics."
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Advanced risk analysis\\nif results:\\n    returns = results['returns']\\n    \\n    print(\\\"âš ï¸ Risk Analysis Dashboard\\\")\\n    print(\\\"=\\\" * 30)\\n    \\n    # Value at Risk (VaR) analysis\\n    var_95 = np.percentile(returns, 5)\\n    var_99 = np.percentile(returns, 1)\\n    cvar_95 = returns[returns <= var_95].mean()\\n    \\n    print(f\\\"ğŸ“Š Value at Risk:\\\")\\n    print(f\\\"   Daily VaR (95%): {var_95:.3%}\\\")\\n    print(f\\\"   Daily VaR (99%): {var_99:.3%}\\\")\\n    print(f\\\"   Conditional VaR (95%): {cvar_95:.3%}\\\")\\n    \\n    # Tail risk analysis\\n    extreme_losses = returns[returns < np.percentile(returns, 5)]\\n    extreme_gains = returns[returns > np.percentile(returns, 95)]\\n    \\n    print(f\\\"\\\\nğŸ“ˆ Tail Risk Analysis:\\\")\\n    print(f\\\"   Extreme loss days: {len(extreme_losses)}\\\")\\n    print(f\\\"   Average extreme loss: {extreme_losses.mean():.3%}\\\")\\n    print(f\\\"   Extreme gain days: {len(extreme_gains)}\\\")\\n    print(f\\\"   Average extreme gain: {extreme_gains.mean():.3%}\\\")\\n    print(f\\\"   Gain/Loss ratio: {extreme_gains.mean() / abs(extreme_losses.mean()):.2f}\\\")\\n    \\n    # Monthly and yearly performance breakdown\\n    monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\\n    yearly_returns = returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)\\n    \\n    print(f\\\"\\\\nğŸ“… Period Performance:\\\")\\n    print(f\\\"   Best month: {monthly_returns.max():.2%} ({monthly_returns.idxmax().strftime('%Y-%m')})\\\")\\n    print(f\\\"   Worst month: {monthly_returns.min():.2%} ({monthly_returns.idxmin().strftime('%Y-%m')})\\\")\\n    print(f\\\"   Positive months: {(monthly_returns > 0).sum()}/{len(monthly_returns)} ({(monthly_returns > 0).mean():.1%})\\\")\\n    \\n    print(f\\\"\\\\nğŸ“Š Yearly Performance Breakdown:\\\")\\n    for year, ret in yearly_returns.items():\\n        print(f\\\"   {year.year}: {ret:>8.2%}\\\")"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Advanced factor importance analysis\nif results and 'factor_data' in results:\n    factor_data = results['factor_data']\n    \n    print(\"ğŸ“Š Factor Importance Analysis\")\n    print(\"=\" * 35)\n    \n    # Calculate factor importance using correlation with future returns\n    factor_cols = ['momentum_rank', 'value_rank', 'quality_rank', 'size_rank', 'low_vol_rank']\n    \n    # Calculate next period returns\n    factor_data['next_return'] = factor_data.groupby('ticker')['returns'].shift(-1)\n    \n    # Calculate correlations and information coefficients\n    factor_importance = {}\n    for factor in factor_cols:\n        if factor in factor_data.columns:\n            # Pearson correlation\n            corr = factor_data[factor].corr(factor_data['next_return'])\n            # Spearman rank correlation (more robust)\n            rank_corr = factor_data[factor].corr(factor_data['next_return'], method='spearman')\n            \n            factor_name = factor.replace('_rank', '')\n            factor_importance[factor_name] = {\n                'Pearson_Corr': abs(corr),\n                'Spearman_Corr': abs(rank_corr),\n                'Average': (abs(corr) + abs(rank_corr)) / 2\n            }\n    \n    # Create importance DataFrame\n    importance_df = pd.DataFrame(factor_importance).T\n    importance_df = importance_df.sort_values('Average', ascending=False)\n    \n    print(\"ğŸ”¬ Factor Predictive Power (Correlation with Future Returns):\")\n    display(importance_df)\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Factor importance bar chart\n    importance_df['Average'].plot(kind='bar', ax=axes[0], alpha=0.7, color='steelblue')\n    axes[0].set_title('ğŸ“Š Factor Importance (Predictive Power)')\n    axes[0].set_xlabel('Factor')\n    axes[0].set_ylabel('Average Correlation')\n    axes[0].tick_params(axis='x', rotation=45)\n    axes[0].grid(True, alpha=0.3)\n    \n    # Correlation comparison\n    x = np.arange(len(importance_df))\n    width = 0.35\n    \n    axes[1].bar(x - width/2, importance_df['Pearson_Corr'], width, \n               label='Pearson', alpha=0.8, color='lightblue')\n    axes[1].bar(x + width/2, importance_df['Spearman_Corr'], width, \n               label='Spearman', alpha=0.8, color='orange')\n    \n    axes[1].set_xlabel('Factor')\n    axes[1].set_ylabel('Correlation (Absolute)')\n    axes[1].set_title('ğŸ“ˆ Correlation Methods Comparison')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(importance_df.index, rotation=45)\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Factor insights\n    best_factor = importance_df.index[0]\n    worst_factor = importance_df.index[-1]\n    \n    print(f\"\\nğŸ† Key Insights:\")\n    print(f\"   ğŸ¥‡ Most predictive factor: {best_factor.title()}\")\n    print(f\"   ğŸ“Š Predictive power: {importance_df.loc[best_factor, 'Average']:.4f}\")\n    print(f\"   ğŸ¥‰ Least predictive factor: {worst_factor.title()}\")\n    print(f\"   ğŸ“‰ Predictive power: {importance_df.loc[worst_factor, 'Average']:.4f}\")\n    print(f\"   ğŸ“ˆ Factor spread: {importance_df['Average'].max() - importance_df['Average'].min():.4f}\")"
    ]
    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## 7. Advanced Techniques {#advanced}\\n\\nExplore regularization methods and sensitivity analysis."
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Compare regularization techniques\nprint(\"ğŸ”¬ Advanced Regularization Comparison\")\nprint(\"=\" * 45)\n\n# Test different regularization configurations\nconfigs = [\n    {'name': 'No Regularization', 'shrinkage': False, 'lasso': False},\n    {'name': 'Bayesian Shrinkage Only', 'shrinkage': True, 'lasso': False},\n    {'name': 'Lasso Only', 'shrinkage': False, 'lasso': True},\n    {'name': 'Both Techniques', 'shrinkage': True, 'lasso': True}\n]\n\ncomparison_results = []\n\nfor config_dict in configs:\n    print(f\"ğŸ”„ Testing: {config_dict['name']}...\")\n    \n    result = backtester.run_backtest(\n        tickers=tickers[:50],  # Smaller universe for speed\n        use_shrinkage=config_dict['shrinkage'],\n        use_lasso=config_dict['lasso']\n    )\n    \n    if result:\n        comparison_results.append({\n            'Strategy': config_dict['name'],\n            'Total Return': result['metrics']['total_return'],\n            'Annualized Return': result['metrics']['annualized_return'],\n            'Volatility': result['metrics']['volatility'],\n            'Sharpe Ratio': result['metrics']['sharpe_ratio'],\n            'Max Drawdown': result['metrics']['max_drawdown'],\n            'Win Rate': result['metrics']['win_rate']\n        })\n\n# Display comparison results\nif comparison_results:\n    comparison_df = pd.DataFrame(comparison_results)\n    print(\"\\nğŸ“Š Regularization Comparison Results:\")\n    display(comparison_df)\n    \n    # Find best performing strategy\n    best_strategy = comparison_df.loc[comparison_df['Sharpe Ratio'].idxmax()]\n    print(f\"\\nğŸ† Best Strategy: {best_strategy['Strategy']}\")\n    print(f\"   âš¡ Sharpe Ratio: {best_strategy['Sharpe Ratio']:.3f}\")\n    print(f\"   ğŸ“ˆ Annualized Return: {best_strategy['Annualized Return']:.2%}\")"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Final summary and conclusions\nprint(\"\\nğŸ¯ AlphaForge Analysis Summary\")\nprint(\"=\" * 40)\n\nif results:\n    final_sharpe = results['metrics']['sharpe_ratio']\n    final_return = results['metrics']['annualized_return']\n    final_vol = results['metrics']['volatility']\n    final_dd = results['metrics']['max_drawdown']\n    \n    print(f\"ğŸ“Š Final Strategy Performance:\")\n    print(f\"   ğŸ“ˆ Annualized Return: {final_return:.2%}\")\n    print(f\"   ğŸ“‰ Volatility: {final_vol:.2%}\")\n    print(f\"   âš¡ Sharpe Ratio: {final_sharpe:.3f}\")\n    print(f\"   ğŸ”» Max Drawdown: {final_dd:.2%}\")\n    \n    # Strategy assessment\n    if final_sharpe > 1.0:\n        assessment = \"ğŸ† Excellent strategy with strong risk-adjusted returns\"\n    elif final_sharpe > 0.5:\n        assessment = \"âœ… Good strategy with solid performance\"\n    elif final_sharpe > 0.0:\n        assessment = \"âš ï¸ Moderate strategy with room for improvement\"\n    else:\n        assessment = \"âŒ Poor strategy requiring significant revision\"\n    \n    print(f\"\\nğŸ¯ Assessment: {assessment}\")\n    \n    if oos_results:\n        oos_sharpe = oos_results['oos_metrics']['sharpe_ratio']\n        degradation = (oos_sharpe - final_sharpe) / abs(final_sharpe) * 100\n        print(f\"\\nğŸ”¬ Out-of-Sample Validation:\")\n        print(f\"   ğŸ“Š OOS Sharpe Ratio: {oos_sharpe:.3f}\")\n        print(f\"   ğŸ“‰ Performance Degradation: {degradation:.1f}%\")\n        \n        if degradation > -20:\n            validation = \"âœ… Strategy passes out-of-sample validation\"\n        else:\n            validation = \"âš ï¸ Strategy shows significant overfitting\"\n        print(f\"   ğŸ¯ Validation: {validation}\")\n\nprint(f\"\\nğŸš€ Next Steps:\")\nprint(f\"   ğŸ“ˆ Experiment with custom factors using examples/custom_factors.py\")\nprint(f\"   ğŸ”¬ Explore ML techniques for enhanced alpha generation\")\nprint(f\"   ğŸ“Š Test on different universes and time periods\")\nprint(f\"   ğŸ¯ Implement risk management overlays\")\nprint(f\"   ğŸ’° Consider transaction cost optimization\")\n\nprint(f\"\\nâœ¨ AlphaForge analysis complete! Ready to forge alpha! ğŸ”¥\")"
    ]
    },
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## Summary and Conclusions\n",
        "\n",
        "This notebook demonstrates the comprehensive capabilities of **AlphaForge** - a systematic alpha research platform:\n",
        "\n",
        "### ğŸ¯ Key Features Demonstrated:\n",
        "1. **ğŸ”— Data Integration**: Seamless multi-source data fetching with intelligent caching\n",
        "2. **âš™ï¸ Factor Engineering**: Classic factors (momentum, value, quality, size, low-vol) with robust calculation\n",
        "3. **ğŸª Portfolio Construction**: Long-short strategies with position limits and risk controls\n",
        "4. **ğŸ§  Advanced Techniques**: Bayesian shrinkage and Lasso regularization for overfitting prevention\n",
        "5. **ğŸ”¬ Rigorous Testing**: Walk-forward analysis for unbiased out-of-sample validation\n",
        "6. **ğŸ“Š Comprehensive Analytics**: Risk metrics, performance attribution, and sensitivity analysis\n",
        "\n",
        "### ğŸš€ Next Steps for Research:\n",
        "- **Custom Factors**: Develop proprietary signals using the extensible framework\n",
        "- **ML Integration**: Implement ensemble methods and deep learning models\n",
        "- **Alternative Data**: Incorporate sentiment, satellite, and patent data\n",
        "- **Multi-Asset**: Extend to fixed income, commodities, and currencies\n",
        "- **Regime Models**: Add market regime detection and adaptive strategies\n",
        "\n",
        "### ğŸ’¼ Framework Benefits:\n",
        "- **ğŸ—ï¸ Modular Design**: Easy to extend and customize for specific research needs\n",
        "- **âš¡ Performance**: Parallel processing and caching for institutional-scale research\n",
        "- **ğŸ›¡ï¸ Robust**: Transaction costs, survivorship bias, and statistical validation\n",
        "- **ğŸ“ˆ Research-Ready**: Publication-quality analytics and visualizations\n",
        "\n",
        "### ğŸ”¥ Production Applications:\n",
        "- **Hedge Funds**: Systematic strategy development and risk management\n",
        "- **Asset Managers**: Portfolio optimization and performance attribution\n",
        "- **Risk Teams**: Factor exposure monitoring and stress testing\n",
        "- **Academic Research**: Empirical asset pricing and factor model validation\n",
        "\n",
        "**AlphaForge** provides a professional-grade foundation for systematic alpha research that scales from academic studies to institutional trading strategies.\n",
    "\n",
    "---\n",
    "\n",
       "ğŸš€ **Ready to forge alpha?** Explore the examples directory for advanced use cases and custom factor development patterns.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}





